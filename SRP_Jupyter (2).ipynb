{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "036b1523",
   "metadata": {},
   "source": [
    "# SRP 2021 Prof. Gil Eckert, Isabella Chiaravalloti, and Anna Nardelli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c770d72",
   "metadata": {},
   "source": [
    "## Programs that receive ABET accreditation meet certain quality standards that assure the program’s graduates are prepared for employment in computer science careers as they enter the workforce. Part of the accreditation process seeks to affirm that the courses taught reflect the advertised curriculum and embody the ABET accreditation outcomes. This project sought to develop a metric to gauge the level of alignment between course syllabi from the computer science (CS) curriculum at Monmouth University (MU) to descriptions found in the MU course catalog and to ABET computer science accreditation outcomes. A method called Natural Language Processing (NLP) was used to analyze these documents for each course in the program. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24270426",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3657532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import requests\n",
    "import re\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f719bb4",
   "metadata": {},
   "source": [
    "Our imports were used to access, store, and analyze the data used in our program. Below you will find a basic description of how each import was used in the project.\n",
    "\n",
    "-pickle: to store large lists to be called later in the program\n",
    "\n",
    "-nltk: NLP library, used to tokenize data (separate each word into elements of a list) and remove stop words\n",
    "\n",
    "-requests: to access the documents from GitHub into the program\n",
    "\n",
    "-matplotlib: makes charts using gathered data\n",
    "\n",
    "-pandas: makes data frames/series/charts using gathered data\n",
    "\n",
    "-numpy: for large lists and mathematical functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b818b723",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8847bf69",
   "metadata": {},
   "source": [
    "The wordFrequency function finds the words that the syllabus/course description have in common and creates a word frequency chart for each course:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8e7773d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordFrequency(syl, cc, string, count):\n",
    "    #new lists for each doc and one for common words\n",
    "    data = []\n",
    "    syllabus = []\n",
    "    catalog = []\n",
    "    \n",
    "    #finds common words and inserts them into a list with each common word ocurring only once\n",
    "    for word in cc:\n",
    "        if word in syl:\n",
    "            data.append(word)\n",
    "    for word in syl:\n",
    "        if word in data:\n",
    "            syllabus.append(word)     \n",
    "    for word in cc:\n",
    "        if word in data:\n",
    "            catalog.append(word)\n",
    "            \n",
    "    #creates pandas series for each doc and sorts by index\n",
    "    seriesSyl = pd.Series(syllabus)\n",
    "    seriesCC = pd.Series(catalog)\n",
    "    syllabusCount = seriesSyl.value_counts().sort_index()\n",
    "    catalogCount = seriesCC.value_counts().sort_index()\n",
    "    #print(syllabusCount)\n",
    "    #print(catalogCount)\n",
    "    \n",
    "    #sets maximum, which will determine chart height\n",
    "    maximum = 0\n",
    "    max1 = max(syllabusCount)\n",
    "    max2 = max(catalogCount)\n",
    "    if max1 > max2:\n",
    "        maximum = max1\n",
    "    else:\n",
    "        maximum = max2\n",
    "   \n",
    "    #sets up chart output files\n",
    "    fileName = \" \"\n",
    "    if string == \"outcomes\":\n",
    "        legendLabel = \"Outcomes\"\n",
    "        title = courses[count] + \" Syllabus vs. Outcomes\"\n",
    "        wordSimilarity(syllabusCount, catalogCount, \"outcomes\")\n",
    "        fileName = courses[count] + 'Outcomes.png'\n",
    "    else:\n",
    "        legendLabel = \"Course Catalog\"\n",
    "        title = courses[count] + \" Syllabus vs. Course Catalog\"\n",
    "        wordSimilarity(syllabusCount, catalogCount, \"cc\")\n",
    "        fileName = courses[count] + 'CC.png'\n",
    "    \n",
    "    #creates charts and shows/saves to file\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.plot(syllabusCount, label = \"Syllabus\", color = colors[0])\n",
    "    plt.plot(catalogCount, label = legendLabel, color = colors[1])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel(\"Words\")\n",
    "    plt.yticks(np.arange(1, maximum+1, 1))\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.title(title)\n",
    "    plt.savefig(fileName)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdfe137",
   "metadata": {},
   "source": [
    "The wordSimilarity function is used to execute the mathematical function that determines the similarity corellation score for each course:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3f37f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordSimilarity(syl, cc, string):\n",
    "    #WORD SIMILARITY = (CC WORD COUNT / CC TOTAL WORD COUNT) * (LESSER OF SYL WORD COUNT AND CC WORD COUNT)\n",
    "    syl = syl.tolist()\n",
    "    cc = cc.tolist()\n",
    "    total = 0\n",
    "    lesser = 0\n",
    "    ccLength = 0\n",
    "    count = 0\n",
    "    for ccWord in cc:\n",
    "        ccLength += ccWord\n",
    "    for ccWord in cc:\n",
    "        if syl[count] < ccWord:\n",
    "            lesser = syl[count]\n",
    "        else:\n",
    "            lesser = ccWord\n",
    "        count+=1\n",
    "        wordSim = ((ccWord/(ccLength)) * (lesser))\n",
    "        total = wordSim + total\n",
    "    #separates Outcomes data from CC data, allowing this function to be used for both types of documents\n",
    "    if string == \"outcomes\":\n",
    "        wordSimOut.append(total)\n",
    "    else:\n",
    "        wordSimCC.append(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdca8f23",
   "metadata": {},
   "source": [
    "The descSplit function cleans the data from each course description. It tokenizes the data, makes all words lowercase, removes punctuation and other non-alphabetic characters, and filters out stop words. It also returns the list of clean data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81f3df98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descSplit(desc):\n",
    "    descText = \" \".join(desc)\n",
    "    tokens = word_tokenize(descText) \n",
    "    #convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    #remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    #remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    #filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    new_stopwords = ['ł']\n",
    "    new_stopwords_list = stop_words.union(new_stopwords)\n",
    "    words = [w for w in words if not w in new_stopwords_list]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb34dad",
   "metadata": {},
   "source": [
    "The desc function is used to extract the course description data needed from the Monmouth University Course Catalog. This document is over 600 pages, and this code automated the process of searching the document and pulling out only the data we needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11e3cdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def desc(cc, start):\n",
    "    desc = []\n",
    "    desc.append(cc[start])\n",
    "    desc.append(cc[start + 1])\n",
    "    start += 2\n",
    "    while not cc[start].startswith(\"CS-\"):\n",
    "        desc.append(cc[start])\n",
    "        start += 1\n",
    "    desc = descSplit(desc)\n",
    "    return desc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cbac84",
   "metadata": {},
   "source": [
    "### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd19e61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "courses = ['CS-102', 'CS-104', 'CS-175', 'CS-175L', 'CS-176', 'CS-176L', 'CS-205', 'CS-205L', 'CS-286', 'CS-305', 'CS-310', 'CS-325', 'CS-414', 'CS-418', 'CS-432', 'CS-450', 'CS-490', 'CS-492A']\n",
    "urlList = [l.replace('-','') for l in courses]\n",
    "\n",
    "colors = [\"#4285f4\", \"#ea4335\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8a85a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SYLLABUS\n",
    "sylList = []\n",
    "\n",
    "#for loop takes each course and executes these steps separately for each syllabus\n",
    "for u in urlList:\n",
    "    \n",
    "    #pulls data from the URL and writes it into a string\n",
    "    sylURL = \"https://raw.githubusercontent.com/annanardelli/SRP2021/main/CS%20ABET/\" + u + \".txt\"\n",
    "    sylPage = requests.get(sylURL)\n",
    "    sylData = sylPage.text\n",
    "\n",
    "    if \"\\n\" in sylData:\n",
    "        sylData = sylData.replace(\"\\r\", \"\")\n",
    "    else:\n",
    "        sylData = sylData.replace(\"\\r\", \"\\n\")\n",
    "\n",
    "    #tokenizes the syllabi\n",
    "    tokens = word_tokenize(sylData) \n",
    "    #convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    #remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    #remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    #filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    new_stopwords = ['ł']\n",
    "    new_stopwords_list = stop_words.union(new_stopwords)\n",
    "    words = [w for w in words if not w in new_stopwords_list]\n",
    "    sylList.append(words)\n",
    "    \n",
    "    #pickles the cleaned syllabus into a .txt file\n",
    "    file_name = u + \"Syllabus.txt\"\n",
    "    open_file = open(file_name, \"wb\")\n",
    "    pickle.dump(words, open_file)\n",
    "    open_file.close()\n",
    "    open_file = open(file_name, \"rb\")\n",
    "    loaded_list = pickle.load(open_file)\n",
    "    open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a835f4e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wordSimCC' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-84a782e5d6a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcourseLength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mwordFrequency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msylList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdescList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"cc\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-43e445205702>\u001b[0m in \u001b[0;36mwordFrequency\u001b[0;34m(syl, cc, string, count)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mlegendLabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Course Catalog\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcourses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" Syllabus vs. Course Catalog\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mwordSimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyllabusCount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatalogCount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mfileName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcourses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'CC.png'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-94dc82222cef>\u001b[0m in \u001b[0;36mwordSimilarity\u001b[0;34m(syl, cc, string)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mwordSimOut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mwordSimCC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'wordSimCC' is not defined"
     ]
    }
   ],
   "source": [
    "#COURSE CATALOG\n",
    "\n",
    "#pulls data from the URL and writes it into a string\n",
    "cataURL = \"https://raw.githubusercontent.com/annanardelli/SRP2021/main/UndergraduateCourseCatalog.txt\"\n",
    "cataPage = requests.get(cataURL)\n",
    "cataData = cataPage.text\n",
    "cataData = cataData.splitlines()\n",
    "\n",
    "#runs a search of the CC document for each relevant course description\n",
    "descList = []\n",
    "course = 0\n",
    "for index, item in enumerate(cataData):\n",
    "    #print(index, item)\n",
    "    if item.startswith(courses[course]):\n",
    "        if (\"Credits: \" in cataData[index + 1]) or (\"Credits: \" in cataData[index]):\n",
    "            descList.append(desc(cataData, index)) \n",
    "        if course < (len(courses) - 2):\n",
    "            course += 1\n",
    "\n",
    "count = -1\n",
    "courseLength = len(courses)\n",
    "for i in range(courseLength):\n",
    "    count += 1\n",
    "    wordFrequency(sylList[i],descList[i],\"cc\",count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c6ad22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OUTCOMES\n",
    "\n",
    "#pulls data from the URL and writes it into a string\n",
    "outcomesURL = \"https://raw.githubusercontent.com/annanardelli/SRP2021/main/CSSEOutcomesText/CSOutcomesForSRP.txt\"\n",
    "outcomesPage = requests.get(outcomesURL)\n",
    "outcomesData = outcomesPage.text\n",
    "outcomesList = []\n",
    "\n",
    "tokens = word_tokenize(outcomesData) \n",
    "#convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "#remove punctuation from each word\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "#remove remaining tokens that are not alphabetic\n",
    "words = [word for word in stripped if word.isalpha()]\n",
    "#filter out stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "new_stopwords = ['ł']\n",
    "new_stopwords_list = stop_words.union(new_stopwords)\n",
    "words = [w for w in words if not w in new_stopwords_list]\n",
    "outcomesList.append(words)      \n",
    "count = -1\n",
    "for i in range(courseLength):\n",
    "    count += 1\n",
    "    wordFrequency(sylList[i],outcomesList[0], \"outcomes\", count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dafcb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHARTS\n",
    "plt.figure(figsize=(20,8)) \n",
    "x_axis = np.arange(len(courses))\n",
    "  \n",
    "plt.bar(x_axis - 0.2, wordSimCC, 0.35, label = 'Syllabus vs. CC', align = 'center', color = colors[0])\n",
    "plt.bar(x_axis + 0.2, wordSimOut, 0.35, label = 'Syllabus vs. Outcomes', align = 'center', color = colors[1])\n",
    "  \n",
    "plt.xticks(x_axis, courses)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Courses\")\n",
    "plt.ylabel(\"Word Similarity Measure\")\n",
    "plt.title(\"Word Similarity for Each Course\")\n",
    "plt.legend()\n",
    "plt.savefig('WordSimilarity.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a536ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AVERAGE\n",
    "wordSimAverages = []\n",
    "count = 0\n",
    "for i in wordSimCC:\n",
    "    avg = 0\n",
    "    total = 0\n",
    "    total += i\n",
    "    total += wordSimOut[count]\n",
    "    count += 1\n",
    "    avg = total / 2\n",
    "    wordSimAverages.append(avg)\n",
    "\n",
    "df = pd.DataFrame(wordSimAverages, index=courses, columns= ['Average'])\n",
    "df = df.sort_values(by=['Average'],ascending=False)\n",
    "#print(df)\n",
    "x = df.plot.barh(rot=0, figsize=(12,8), title = \"Average Word Similarity for Each Course\", xticks = df['Average'], width = 0.7, color = colors)\n",
    "x.set_ylabel(\"Course\")\n",
    "x.set_xlabel(\"Average Word Similarity Measure\")\n",
    "maxVal = max(df['Average']+0.1)\n",
    "x.xaxis.set_ticks(np.arange(0, maxVal, 0.2))\n",
    "x.get_legend().remove()\n",
    "fig = x.get_figure()\n",
    "fig.savefig('AverageWordSimilarity.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae6e0f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14eb4a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
